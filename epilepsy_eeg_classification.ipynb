{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV_8KXT81D8C"
      },
      "source": [
        "# Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hyqCcL95ZOV",
        "outputId": "408f25f9-4966-4605-8eb8-dd9a14056eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.7.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.4)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from mne) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (24.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.8.1)\n",
            "Requirement already satisfied: scipy>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from mne) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2024.2.2)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-1.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n4m_ccq51RB7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import kurtosis, skew\n",
        "from scipy.signal import argrelextrema, welch\n",
        "from scipy.integrate import cumtrapz\n",
        "import statistics\n",
        "import time\n",
        "\n",
        "import mne # to read the .edf files\n",
        "import csv # to write out results\n",
        "from datetime import date\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, VarianceThreshold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "egoYutyvP-na"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR-jTHI_1Jsz"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQcCJ1nl08Hy"
      },
      "outputs": [],
      "source": [
        "# some pre-processing used function from pyeeg\n",
        "\n",
        "def bin_power(X, Band, Fs):\n",
        "    \"\"\"Compute power in each frequency bin specified by Band from FFT result of\n",
        "\n",
        "    Parameters\n",
        "    -----------\n",
        "\n",
        "    X: list: a 1-D real time series.\n",
        "    Band; list\n",
        "        boundary frequencies (in Hz) of bins. They can be unequal bins, e.g.\n",
        "        [0.5,4,7,12,30] which are delta, theta, alpha and beta respectively.\n",
        "        You can also use range() function of Python to generate equal bins and\n",
        "        pass the generated list to this function.\n",
        "\n",
        "        Each element of Band is a physical frequency and shall not exceed the\n",
        "        Nyquist frequency, i.e., half of sampling frequency.\n",
        "\n",
        "\n",
        "\n",
        "    Fs: integer: the sampling rate in physical frequency\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    Power: list: spectral power in each frequency bin.\n",
        "\n",
        "    Power_ratio: list:\n",
        "        spectral power in each frequency bin normalized by total power in ALL\n",
        "        frequency bins.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    C = np.fft.fft(X)\n",
        "    C = abs(C)\n",
        "    Power = np.zeros(len(Band) - 1)\n",
        "    for Freq_Index in range(0, len(Band) - 1):\n",
        "        Freq = float(Band[Freq_Index])\n",
        "        Next_Freq = float(Band[Freq_Index + 1])\n",
        "        Power[Freq_Index] = sum(\n",
        "            C[int(np.floor(Freq / Fs * len(X))):\n",
        "                int(np.floor(Next_Freq / Fs * len(X)))]\n",
        "        )\n",
        "    Power_Ratio = Power / sum(Power)\n",
        "    return Power, Power_Ratio\n",
        "\n",
        "\n",
        "def hfd(X, Kmax):\n",
        "    \"\"\" Compute Higuchi Fractal Dimension of a time series X. kmax\n",
        "     is an HFD parameter\n",
        "    \"\"\"\n",
        "    L = []\n",
        "    x = []\n",
        "    N = len(X)\n",
        "    for k in range(1, Kmax):\n",
        "        Lk = []\n",
        "        for m in range(0, k):\n",
        "            Lmk = 0\n",
        "            for i in range(1, int(np.floor((N - m) / k))):\n",
        "                Lmk += abs(X[m + i * k] - X[m + i * k - k])\n",
        "            Lmk = Lmk * (N - 1) / np.floor((N - m) / float(k)) / k\n",
        "            Lk.append(Lmk)\n",
        "        L.append(np.log(np.mean(Lk)))\n",
        "        x.append([np.log(float(1) / k), 1])\n",
        "\n",
        "    (p, _, _, _) = np.linalg.lstsq(x, L)\n",
        "    return p[0]\n",
        "\n",
        "\n",
        "\n",
        "def pfd(X, D=None):\n",
        "    \"\"\"Compute Petrosian Fractal Dimension of a time series from either two\n",
        "    cases below:\n",
        "        1. X, the time series of type list (default)\n",
        "        2. D, the first order differential sequence of X (if D is provided,\n",
        "           recommended to speed up)\n",
        "\n",
        "    In case 1, D is computed using Numpy's difference function.\n",
        "\n",
        "    To speed up, it is recommended to compute D before calling this function\n",
        "    because D may also be used by other functions whereas computing it here\n",
        "    again will slow down.\n",
        "    \"\"\"\n",
        "    if D is None:\n",
        "        D = np.diff(X)\n",
        "        D = D.tolist()\n",
        "    N_delta = 0  # number of sign changes in derivative of the signal\n",
        "    for i in range(1, len(D)):\n",
        "        if D[i] * D[i - 1] < 0:\n",
        "            N_delta += 1\n",
        "    n = len(X)\n",
        "    return np.log10(n) / (\n",
        "        np.log10(n) + np.log10(n / n + 0.4 * N_delta)\n",
        "    )\n",
        "\n",
        "\n",
        "def hurst(X):\n",
        "    \"\"\" Compute the Hurst exponent of X. If the output H=0.5,the behavior\n",
        "    of the time-series is similar to random walk. If H<0.5, the time-series\n",
        "    cover less \"distance\" than a random walk, vice verse.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    X: list: a time series\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    H: float:  Hurst exponent\n",
        "    \"\"\"\n",
        "    X = np.array(X)\n",
        "    N = X.size\n",
        "    T = np.arange(1, N + 1)\n",
        "    Y = np.cumsum(X)\n",
        "    Ave_T = Y / T\n",
        "\n",
        "    S_T = np.zeros(N)\n",
        "    R_T = np.zeros(N)\n",
        "\n",
        "    for i in range(N):\n",
        "        S_T[i] = np.std(X[:i + 1])\n",
        "        X_T = Y - T * Ave_T[i]\n",
        "        R_T[i] = np.ptp(X_T[:i + 1])\n",
        "\n",
        "    R_S = R_T / S_T\n",
        "    R_S = np.log(R_S)[1:]\n",
        "    n = np.log(T)[1:]\n",
        "    A = np.column_stack((n, np.ones(n.size)))\n",
        "    [m, c] = np.linalg.lstsq(A, R_S)[0]\n",
        "    H = m\n",
        "    return H\n",
        "\n",
        "\n",
        "def spectral_entropy(X, Band, Fs, Power_Ratio=None):\n",
        "    \"\"\"Compute spectral entropy of a time series from either two cases below:\n",
        "    1. X, the time series (default)\n",
        "    2. Power_Ratio, a list of normalized signal power in a set of frequency\n",
        "    bins defined in Band (if Power_Ratio is provided, recommended to speed up)\n",
        "\n",
        "    In case 1, Power_Ratio is computed by bin_power() function.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    To speed up, it is recommended to compute Power_Ratio before calling this\n",
        "    function because it may also be used by other functions whereas computing\n",
        "    it here again will slow down.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    X: list: a 1-D real time series.\n",
        "\n",
        "    Band: list\n",
        "\n",
        "        boundary frequencies (in Hz) of bins. They can be unequal bins, e.g.\n",
        "        [0.5,4,7,12,30] which are delta, theta, alpha and beta respectively.\n",
        "        You can also use range() function of Python to generate equal bins and\n",
        "        pass the generated list to this function.\n",
        "\n",
        "        Each element of Band is a physical frequency and shall not exceed the\n",
        "        Nyquist frequency, i.e., half of sampling frequency.\n",
        "\n",
        "\n",
        "    Fs: integer: the sampling rate in physical frequency\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    As indicated in return line\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    bin_power: pyeeg function that computes spectral power in frequency bins\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if Power_Ratio is None:\n",
        "        Power, Power_Ratio = bin_power(X, Band, Fs)\n",
        "\n",
        "    Spectral_Entropy = 0\n",
        "    for i in range(0, len(Power_Ratio) - 1):\n",
        "        Spectral_Entropy += Power_Ratio[i] * np.log(Power_Ratio[i])\n",
        "    Spectral_Entropy /= np.log(\n",
        "        len(Power_Ratio)\n",
        "    )  # to save time, minus one is omitted\n",
        "    return -1 * Spectral_Entropy\n",
        "\n",
        "\n",
        "def hjorth(X, D=None):\n",
        "    \"\"\" Compute Hjorth mobility and complexity of a time series from either two\n",
        "    cases below:\n",
        "        1. X, the time series of type list (default)\n",
        "        2. D, a first order differential sequence of X (if D is provided,\n",
        "           recommended to speed up)\n",
        "\n",
        "    In case 1, D is computed using Numpy's Difference function.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    To speed up, it is recommended to compute D before calling this function\n",
        "    because D may also be used by other functions whereas computing it here\n",
        "    again will slow down.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    X: list: a time series\n",
        "\n",
        "    D: list: first order differential sequence of a time series\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    As indicated in return line\n",
        "\n",
        "    Hjorth mobility and complexity\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if D is None:\n",
        "        D = np.diff(X)\n",
        "        D = D.tolist()\n",
        "\n",
        "    D.insert(0, X[0])  # pad the first difference\n",
        "    D = np.array(D)\n",
        "\n",
        "    n = len(X)\n",
        "\n",
        "    M2 = float(sum(D ** 2)) / n\n",
        "    TP = sum(np.array(X) ** 2)\n",
        "    M4 = 0\n",
        "    for i in range(1, len(D)):\n",
        "        M4 += (D[i] - D[i - 1]) ** 2\n",
        "    M4 = M4 / n\n",
        "\n",
        "    return np.sqrt(M2 / TP), np.sqrt(\n",
        "        float(M4) * TP / M2 / M2\n",
        "    )  # Hjorth Mobility and Complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f0xHt8q3ki5"
      },
      "outputs": [],
      "source": [
        "# read the edf and print stuff first\n",
        "# def eeg_visualize(file, start_time, end_time):\n",
        "def eeg_visualize(file):\n",
        "    raw = mne.io.read_raw_edf(file)\n",
        "    n = 2\n",
        "\n",
        "    # MNE-Python's interactive data browser to get a better visualization\n",
        "    raw.plot()\n",
        "\n",
        "    # select a time frame\n",
        "    start, stop = raw.time_as_index([100, 115])  # 100 s to 115 s data segment\n",
        "    temp, times = raw[:, start:stop]\n",
        "    fig, axs = plt.subplots(n)\n",
        "    fig.suptitle('Patient EEG')\n",
        "    plt.xlabel('time (s)')\n",
        "    plt.ylabel('MEG data (T)')\n",
        "    for i in range(n):\n",
        "        axs[i].plot(times, temp[i].T)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-dV0mbRBGms"
      },
      "outputs": [],
      "source": [
        "# feature extracting process\n",
        "def eeg_features(data):\n",
        "    data = np.asarray(data)\n",
        "    res  = np.zeros([22])\n",
        "    Kmax = 5\n",
        "    # M    = 10\n",
        "    # R    = 0.3\n",
        "    Band = [1,5,10,15,20,25]\n",
        "    Fs   = 256\n",
        "    power, power_ratio = bin_power(data, Band, Fs)\n",
        "    f, P = welch(data, fs=Fs, window='hann', noverlap=0, nfft=int(256.))       # Signal power spectrum\n",
        "    area_freq = cumtrapz(P, f, initial=0)\n",
        "    res[0] = np.sqrt(np.sum(np.power(data, 2)) / data.shape[0])                   # amplitude RMS\n",
        "    res[1] = statistics.stdev(data)**2                                            # variance\n",
        "    res[2] = kurtosis(data)                                                       # kurtosis\n",
        "    res[3] = skew(data)                                                           # skewness\n",
        "    res[4] = max(data)                                                            # max amplitude\n",
        "    res[5] = min(data)                                                            # min amplitude\n",
        "    res[6] = len(argrelextrema(data, np.greater)[0])                              # number of local extrema or peaks\n",
        "    res[7] = ((data[:-1] * data[1:]) < 0).sum()                                   # number of zero crossings\n",
        "    res[8] = hfd(data, Kmax)                                                      # Higuchi Fractal Dimension\n",
        "    res[9] = pfd(data)                                                            # Petrosian Fractal Dimension\n",
        "    res[10] = hurst(data)                                                         # Hurst exponent\n",
        "    res[11] = spectral_entropy(data, Band, Fs, Power_Ratio=power_ratio)           # spectral entropy (1.21s)\n",
        "    res[12] = area_freq[-1]                                                       # total power\n",
        "    res[13] = f[np.where(area_freq >= res[12] / 2)[0][0]]                         # median frequency\n",
        "    res[14] = f[np.argmax(P)]                                                     # peak frequency\n",
        "    res[15], res[16] = hjorth(data)                                               # Hjorth mobility and complexity\n",
        "    res[17] = power_ratio[0]\n",
        "    res[18] = power_ratio[1]\n",
        "    res[19] = power_ratio[2]\n",
        "    res[20] = power_ratio[3]\n",
        "    res[21] = power_ratio[4]\n",
        "    # res[22] = pyeeg.samp_entropy(data, M, R)             # sample entropy\n",
        "    # res[23] = pyeeg.ap_entropy(data, M, R)             # approximate entropy (1.14s)\n",
        "    return (res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4gftqbjBT1p"
      },
      "outputs": [],
      "source": [
        "# eeg pre-processing\n",
        "def eeg_preprocessing(file, seizures, epoch_length = 10, step_size = 1, start_time = 0):\n",
        "    start = time.time()\n",
        "\n",
        "    # reading in data\n",
        "    raw = mne.io.read_raw_edf(file)\n",
        "\n",
        "    # apply filterbank\n",
        "    raw = raw.load_data().filter(l_freq=0.25, h_freq=25)\n",
        "    channels = raw.ch_names                                  # column names\n",
        "\n",
        "    # Divide into epochs\n",
        "    res = []\n",
        "    while start_time <= max(raw.times) + 0.01 - epoch_length:  # max(raw.times) = 3600\n",
        "#     while start_time <= 50 - epoch_length:  # max(raw.times) = 3600\n",
        "        features = []\n",
        "        start, stop = raw.time_as_index([start_time, start_time + epoch_length])\n",
        "        temp = raw[:, start:stop][0]\n",
        "\n",
        "        # start time as ID\n",
        "        features.append(start_time)\n",
        "\n",
        "        # features\n",
        "        for i in range(23):\n",
        "            features.extend(eeg_features(temp[i]).tolist())\n",
        "\n",
        "        # seizure flag for y\n",
        "        if file in seizures:  # if file has seizure\n",
        "            for seizure in seizures[file]:\n",
        "                if start_time > seizure[0] and start_time < seizure[1]:\n",
        "                    features.append(1)\n",
        "                elif start_time + epoch_length > seizure[0] and start_time + epoch_length < seizure[1]:\n",
        "                    features.append(1)\n",
        "                else:\n",
        "                    features.append(0)\n",
        "        else:\n",
        "            features.append(0)\n",
        "\n",
        "        res.append(features)\n",
        "        start_time += step_size\n",
        "        print(\"Section \", str(len(res)), \"; start: \", start, \" ; stop: \", stop)\n",
        "\n",
        "    # formatting\n",
        "    feature_names = [\"rms\", \"variance\", \"kurtosis\", \"skewness\", \"max_amp\", \"min_amp\", \"n_peaks\", \"n_crossings\",\n",
        "        \"hfd\", \"pfd\", \"hurst_exp\", \"spectral_entropy\", \"total_power\", \"median_freq\", \"peak_freq\",\n",
        "        \"hjorth_mobility\", \"hjorth_complexity\", \"power_1hz\", \"power_5hz\", \"power_10hz\", \"power_15hz\", \"power_20hz\"]\n",
        "\n",
        "    column_names = [\"start_time\"]\n",
        "    for channel in channels:\n",
        "        for name in feature_names:\n",
        "            column_names.append(channel + \"_\" + name)\n",
        "    column_names.append(\"seizure\")\n",
        "\n",
        "    res = pd.DataFrame(res, columns=column_names)\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Finished preprocessing \", file, f\" took {(end - start) / 60} minutes\")\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDGVNua41X_C"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UQAkKF9-1-v"
      },
      "outputs": [],
      "source": [
        "def pca(res, patient_num):\n",
        "  # more preprocessing -- this is training specific\n",
        "  X = res.loc[:, res.columns != \"seizure\"]\n",
        "  X = X.loc[:, X.columns != \"start_time\"]\n",
        "  X = X.loc[:, X.columns != \"file ID\"]\n",
        "\n",
        "  results = {'# of features': [], 'features_idx': [], 'SVM accuracy': [], 'MLP accuracy': [], 'SVM rates': [], 'MLP rates': []}\n",
        "  writeout = {}\n",
        "  #for n_ft in range(1, len(X.columns)): # This one is to run through all features (~510)\n",
        "  for n_ft in range(1, 3): # This one is for testing\n",
        "    # Apply PCA to X values\n",
        "    scaler = StandardScaler()\n",
        "    scaled_X = scaler.fit_transform(X)\n",
        "    pca = PCA(0.80)\n",
        "    pc_X= pca.fit_transform(scaled_X)\n",
        "    pc_X_df = pd.DataFrame(data = pc_X, columns = ['PC' + str(i) for i in range(1, pca.n_components_ + 1)])\n",
        "    scale_factor = 10\n",
        "    scaled_pc_X = (pc_X * scale_factor).astype(int)\n",
        "    Y = np.asarray(res['seizure'])\n",
        "    feature_names = X.columns.tolist()\n",
        "\n",
        "    # Finding the most important features\n",
        "    explained_variance_ratio = pca.explained_variance_ratio_\n",
        "    loadings = pca.components_ # each row reprensents a principal component\n",
        "    writeout[\"Explained Variance Ratio\"] = [\"Vector of the variance explained by each dimension\", pca.explained_variance_ratio_]\n",
        "\n",
        "    # get the absolute value of the loading\n",
        "    abs_load = np.abs(loadings)\n",
        "    writeout[\"Absolute Loadings\"] = [\"How much each feature influences a principal component (each row is a principal component)\", abs_load]\n",
        "\n",
        "    # get the weight of all the features from the PCA\n",
        "    feature_importance = np.sum(abs_load, axis = 0)\n",
        "\n",
        "    # sort the index of the feature importance in descending order\n",
        "    # biggest features show up first\n",
        "    sorted_idx = np.argsort(feature_importance)[::-1]\n",
        "    writeout[\"Sorted IDX\"] = [\"List of indices of the features, sorted in descending order of importance\", sorted_idx]\n",
        "\n",
        "    # get n_ft most important features\n",
        "    most_important_feature = sorted_idx[:n_ft]\n",
        "    # get the new columns from the PCA\n",
        "    new_columns = []\n",
        "    ft_id = []\n",
        "    for j in most_important_feature:\n",
        "        new_columns.append(feature_names[j])\n",
        "        name = str(feature_names[j])\n",
        "        ft_id.append(f\"{name} (ID: {j})\")\n",
        "    new_df = res[new_columns]\n",
        "    writeout[\"Post-PCA Data\"] = [\"Data used to determine train-test splits (after some scaling/transforming)\", new_df]\n",
        "\n",
        "    # scale new_df\n",
        "    n_scaler = StandardScaler()\n",
        "    n_scaled_X = scaler.fit_transform(new_df)\n",
        "\n",
        "    # quantize to int\n",
        "    scaled_n_X = (n_scaled_X * scale_factor).astype(int)\n",
        "\n",
        "    # change back to df just for visualization\n",
        "    n_scaled_df = pd.DataFrame(data = scaled_n_X, columns = [feature_names[i] for i in most_important_feature])\n",
        "\n",
        "    # used_input = scaled_pc_X\n",
        "    used_input = scaled_n_X\n",
        "    X_train, X_test, y_train, y_test = train_test_split(used_input, Y, test_size = 0.2, random_state=0)\n",
        "    writeout[\"Train-Test Splits\"] = [\"Train-test splits used for SVM and MLP\", [f\"X_train: {X_train}\", f\"X_test: {X_test}\", f\"y_train: {y_train}\", f\"y_test: {y_test}\"]]\n",
        "\n",
        "    steps(n_ft, writeout, patient_num) # Comment out if processing steps file is unwanted (final file will only contain results from last iteration of features)\n",
        "\n",
        "    acc_svm, rates_svm = svm(y_train, y_test, X_train, X_test)\n",
        "    acc_mlp, rates_mlp = mlp(y_train, y_test, X_train, X_test)\n",
        "\n",
        "    # results = {'# of features': [], 'features': [], 'SVM accuracy': [], 'MLP accuracy': []}\n",
        "    results['# of features'].append(n_ft)\n",
        "    results['features_idx'].append(ft_id)\n",
        "    results['SVM accuracy'].append(acc_svm)\n",
        "    results['MLP accuracy'].append(acc_mlp)\n",
        "\n",
        "    # True Negatives + False Positives = False Negatives + True Positives = 1\n",
        "    results['SVM rates'].append(rates_svm)\n",
        "    results['MLP rates'].append(rates_mlp) # TODO: MLP rates seem suspicious, come back and check why\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcbrDnHc9HFf"
      },
      "outputs": [],
      "source": [
        "def svm(y_train, y_test, X_train, X_test):\n",
        "  class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "  class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "  model = SVC(kernel=\"rbf\", class_weight=class_weight_dict, random_state = 0)\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "  accuracy, TPR, FPR = [], [], []\n",
        "  for train, test in kf.split(X_train):\n",
        "    # train\n",
        "    model.fit(X_train[train, :], y_train[train])\n",
        "    pred = model.predict(X_train[test])\n",
        "    tn, fp, fn, tp = confusion_matrix(y_train[test], pred).ravel()\n",
        "    accuracy.append((tp + tn)/(tn + fp + fn + tp))\n",
        "    TPR.append(tp / (tp + fn))\n",
        "    FPR.append(fp / (fp + tn))\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "  pred = model.predict(X_test)\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
        "  test_accuracy = ((tp + tn)/(tn + fp + fn + tp))\n",
        "  test_TPR = (tp / (tp + fn))\n",
        "  test_FPR = (fp / (fp + tn))\n",
        "  test_TNR = (tn / (fp + tn))\n",
        "  test_FNR = (fn / (tp + fn))\n",
        "\n",
        "  total = len(pred)\n",
        "  right = 0\n",
        "  for i in range(len(pred)):\n",
        "      if (pred[i] == y_test[i]):\n",
        "          right += 1\n",
        "  acc= (right/total)*100\n",
        "\n",
        "  return acc, [\"True Positives: \" + str(test_TPR), \"False Positives: \" + str(test_FPR), \"True Negatives: \" + str(test_TNR), \"False Negatives: \" + str(test_FNR)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZvPHOZH-b3i"
      },
      "outputs": [],
      "source": [
        "def mlp(y_train, y_test, X_train, X_test):\n",
        "  model2 = MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
        "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
        "       hidden_layer_sizes=(10, 10), learning_rate='constant',\n",
        "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
        "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
        "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
        "       verbose=False, warm_start=False)\n",
        "\n",
        "  kf = KFold(n_splits=5)\n",
        "  accuracy2, TPR2, FPR2 = [], [], []\n",
        "  for train, test in kf.split(X_train):\n",
        "    # train\n",
        "    model2.fit(X_train[train, :], y_train[train])\n",
        "    pred2 = model2.predict(X_train[test])\n",
        "    tn, fp, fn, tp = confusion_matrix(y_train[test], pred2).ravel()\n",
        "    accuracy2.append((tp + tn)/(tn + fp + fn + tp))\n",
        "    TPR2.append(tp / (tp + fn))\n",
        "    FPR2.append(fp / (fp + tn))\n",
        "\n",
        "  model2.fit(X_train, y_train)\n",
        "  pred2 = model2.predict(X_test)\n",
        "  tn2, fp2, fn2, tp2 = confusion_matrix(y_test, pred2).ravel()\n",
        "  test_accuracy2 = ((tp2 + tn2)/(tn2 + fp2 + fn2 + tp2))\n",
        "  test_TPR2 = (tp2 / (tp2 + fn2))\n",
        "  test_FPR2 = (fp2 / (fp2 + tn2))\n",
        "  #test_accuracy2\n",
        "  test_TNR2 = (tn2 / (fp2 + tn2))\n",
        "  test_FNR2 = (fn2 / (tp2 + fn2))\n",
        "\n",
        "  total = len(pred2)\n",
        "  right = 0\n",
        "  for i in range(len(pred2)):\n",
        "      if (pred2[i] == y_test[i]):\n",
        "          right += 1\n",
        "  acc= (right/total)*100\n",
        "\n",
        "  return acc, [\"True Positives: \" + str(test_TPR2), \"False Positives: \" + str(test_FPR2), \"True Negatives: \" + str(test_TNR2), \"False Negatives: \" + str(test_FNR2)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write select processing steps to a txt file\n",
        "def steps(num, output, patient):\n",
        "  # Set name for file\n",
        "  today = date.today()\n",
        "\n",
        "  # Formatting dividers\n",
        "  div1 = \"-\" * 20\n",
        "  div2 = \"*\" * 50\n",
        "\n",
        "  # Write to file\n",
        "  f = open(f\"ProcessSteps_Patient{patient}_{str(today.strftime('%b-%d'))}.txt\", \"w\")\n",
        "  for step in output:\n",
        "    f.write(step + \"\\n\" + output[step][0] + \"\\n\" + div1 + \"\\n\" + str(output[step][1]) + \"\\n\" + div2 + \"\\n\")\n",
        "  f.close()"
      ],
      "metadata": {
        "id": "_sQYygeYCgd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmPPUElpBcnx"
      },
      "source": [
        "# Reading and Preprocessing files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv(f\"preprocessed_1.csv\", on_bad_lines='warn')\n",
        "data6 = pd.read_csv(f\"preprocessed_6.csv\", on_bad_lines='warn')"
      ],
      "metadata": {
        "id": "WSryu3AIPi6b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(data1[\"seizure\"], axis=0))\n",
        "print(np.unique(data6[\"seizure\"], axis=0))\n",
        "print(data1[\"seizure\"].shape)\n",
        "print(data6[\"seizure\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOo7ra2xPvZm",
        "outputId": "512f5af3-1792-4173-d867-426e1f7b575b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n",
            "[0]\n",
            "(27453,)\n",
            "(14391,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEqB-cj47G7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649a1b59-6fb9-4336-ffa5-7264bc4123bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  start_time  FP1-F7_rms  FP1-F7_variance  FP1-F7_kurtosis  \\\n",
            "0           0           0    0.000062     3.817093e-09         0.720216   \n",
            "1           1           1    0.000054     2.960967e-09         0.624579   \n",
            "2           2           2    0.000054     2.910406e-09         0.563609   \n",
            "3           3           3    0.000054     2.864906e-09         0.606589   \n",
            "4           4           4    0.000053     2.832845e-09         0.656068   \n",
            "\n",
            "   FP1-F7_skewness  FP1-F7_max_amp  FP1-F7_min_amp  FP1-F7_n_peaks  \\\n",
            "0         0.002630        0.000204       -0.000212           160.0   \n",
            "1         0.265297        0.000204       -0.000171           160.0   \n",
            "2         0.295528        0.000204       -0.000171           159.0   \n",
            "3         0.273589        0.000204       -0.000171           159.0   \n",
            "4         0.285109        0.000204       -0.000171           158.0   \n",
            "\n",
            "   FP1-F7_n_crossings  ...  T8-P8-1_median_freq  T8-P8-1_peak_freq  \\\n",
            "0               117.0  ...                  3.0                1.0   \n",
            "1               120.0  ...                  3.0                1.0   \n",
            "2               111.0  ...                  3.0                1.0   \n",
            "3               109.0  ...                  3.0                1.0   \n",
            "4               105.0  ...                  3.0                1.0   \n",
            "\n",
            "   T8-P8-1_hjorth_mobility  T8-P8-1_hjorth_complexity  T8-P8-1_power_1hz  \\\n",
            "0                 0.002580                 159.770187           0.491932   \n",
            "1                 0.002558                 159.008735           0.482888   \n",
            "2                 0.002501                 157.634496           0.477509   \n",
            "3                 0.002500                 161.509743           0.479711   \n",
            "4                 0.002482                 157.314052           0.485689   \n",
            "\n",
            "   T8-P8-1_power_5hz  T8-P8-1_power_10hz  T8-P8-1_power_15hz  \\\n",
            "0           0.235901            0.108240            0.089723   \n",
            "1           0.256244            0.108781            0.082330   \n",
            "2           0.257324            0.111639            0.080919   \n",
            "3           0.258054            0.109729            0.080249   \n",
            "4           0.250842            0.108714            0.082563   \n",
            "\n",
            "   T8-P8-1_power_20hz  seizure  \n",
            "0            0.074204        0  \n",
            "1            0.069758        0  \n",
            "2            0.072609        0  \n",
            "3            0.072258        0  \n",
            "4            0.072192        0  \n",
            "\n",
            "[5 rows x 509 columns]\n"
          ]
        }
      ],
      "source": [
        "patient_nums = [1, 6]\n",
        "for i in patient_nums:\n",
        "  data = pd.read_csv(f\"preprocessed_{i}.csv\", on_bad_lines='warn')\n",
        "  print(data.head())\n",
        "  break\n",
        "  results = pca(data, i)\n",
        "\n",
        "  # Write key results (number of features, feature names and ID, and SVM/MLP accuracy) to CSV file\n",
        "  results_table = pd.DataFrame.from_dict(results, orient='index')\n",
        "  results_table.to_csv(f\"patient{i}_results.csv\", index=False)\n",
        "\n",
        "  # Plot SVM and MLP accuracy as a function of the number of features used\n",
        "  fig = plt.figure(f\"Patient {i} Plot\")\n",
        "  plt.plot(results[\"# of features\"], results[\"SVM accuracy\"], label=\"SVM Accuracy\")\n",
        "  plt.plot(results[\"# of features\"], results[\"MLP accuracy\"], label=\"MLP Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.title(\"SVM vs MLP Seizure Detection Accuracy\")\n",
        "  plt.xlabel(\"Number of Features\")\n",
        "  plt.ylabel('Accuracy Rate (%)')\n",
        "  plt.savefig(f\"patient{i}_accuracies.png\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HV_8KXT81D8C",
        "xR-jTHI_1Jsz"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}